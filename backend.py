

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import os
import warnings
import matplotlib
from collections import Counter
import random
from math import log2, sqrt
import time

warnings.filterwarnings('ignore')
matplotlib.use('Agg')
plt.style.use('dark_background')

class DecisionTreeNode:
    """N√≥ da √Årvore de Decis√£o implementada manualmente"""
    def __init__(self):
        self.feature_index = None
        self.threshold = None
        self.left = None
        self.right = None
        self.value = None  
        self.samples = 0
        self.gini = 0.0
        
    def is_leaf(self):
        return self.value is not None

class ManualDecisionTree:
    """√Årvore de Decis√£o implementada manualmente para detec√ß√£o de fraudes"""
    
    def __init__(self, max_depth=10, min_samples_split=5, min_samples_leaf=2):
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
        self.min_samples_leaf = min_samples_leaf
        self.root = None
        self.feature_importances_ = None
        
    def gini_impurity(self, y):
        """Calcula impureza de Gini"""
        if len(y) == 0:
            return 0
        
        counts = Counter(y)
        total = len(y)
        impurity = 1.0
        
        for count in counts.values():
            prob = count / total
            impurity -= prob ** 2
            
        return impurity
    
    def entropy(self, y):
        """Calcula entropia"""
        if len(y) == 0:
            return 0
            
        counts = Counter(y)
        total = len(y)
        entropy = 0.0
        
        for count in counts.values():
            if count > 0:
                prob = count / total
                entropy -= prob * log2(prob)
                
        return entropy
    
    def information_gain(self, X_feature, y, threshold):
        """Calcula ganho de informa√ß√£o"""
        # Dividir dados
        left_mask = X_feature <= threshold
        right_mask = X_feature > threshold
        
        if np.sum(left_mask) == 0 or np.sum(right_mask) == 0:
            return 0
            
        # Calcular entropia ponderada
        total_samples = len(y)
        left_weight = np.sum(left_mask) / total_samples
        right_weight = np.sum(right_mask) / total_samples
        
        weighted_entropy = (left_weight * self.entropy(y[left_mask]) + 
                           right_weight * self.entropy(y[right_mask]))
        
        return self.entropy(y) - weighted_entropy
    
    def find_best_split(self, X, y):
        best_feature = None
        best_threshold = None
        best_gain = -1
        
        n_features = X.shape[1]
        
        # Testar cada feature
        for feature_idx in range(n_features):
            feature_values = X[:, feature_idx]
            thresholds = np.unique(feature_values)
            
            # Testar diferentes thresholds
            for threshold in thresholds:
                gain = self.information_gain(feature_values, y, threshold)
                
                if gain > best_gain:
                    best_gain = gain
                    best_feature = feature_idx
                    best_threshold = threshold
                    
        return best_feature, best_threshold, best_gain
    
    def build_tree(self, X, y, depth=0):
        node = DecisionTreeNode()
        node.samples = len(y)
        node.gini = self.gini_impurity(y)
        
        # Condi√ß√µes de parada
        if (depth >= self.max_depth or 
            len(y) < self.min_samples_split or 
            len(np.unique(y)) == 1):
            
            # Criar n√≥ folha
            node.value = Counter(y).most_common(1)[0][0]
            return node
        
        # Encontrar melhor divis√£o
        feature_idx, threshold, gain = self.find_best_split(X, y)
        
        if gain == 0:
            # N√£o h√° ganho, criar folha
            node.value = Counter(y).most_common(1)[0][0]
            return node
            
        # Dividir dados
        left_mask = X[:, feature_idx] <= threshold
        right_mask = X[:, feature_idx] > threshold
        
        # Verificar se divis√£o √© v√°lida
        if (np.sum(left_mask) < self.min_samples_leaf or 
            np.sum(right_mask) < self.min_samples_leaf):
            node.value = Counter(y).most_common(1)[0][0]
            return node
        
        # Configurar n√≥ interno
        node.feature_index = feature_idx
        node.threshold = threshold
        
        # Construir sub√°rvores
        node.left = self.build_tree(X[left_mask], y[left_mask], depth + 1)
        node.right = self.build_tree(X[right_mask], y[right_mask], depth + 1)
        
        return node
    
    def fit(self, X, y):
        """Treina a √°rvore de decis√£o"""
        self.root = self.build_tree(X, y)
        self._calculate_feature_importances(X, y)
        
    def _calculate_feature_importances(self, X, y):
        """Calcula import√¢ncia das features"""
        n_features = X.shape[1]
        self.feature_importances_ = np.zeros(n_features)
        
        def traverse(node, total_samples):
            if node.is_leaf():
                return
                
            # Calcular import√¢ncia desta divis√£o
            feature_idx = node.feature_index
            importance = (node.samples / total_samples) * node.gini
            
            left_gini = node.left.gini if node.left else 0
            right_gini = node.right.gini if node.right else 0
            
            left_samples = node.left.samples if node.left else 0
            right_samples = node.right.samples if node.right else 0
            
            weighted_child_gini = ((left_samples * left_gini + right_samples * right_gini) / 
                                  node.samples)
            
            self.feature_importances_[feature_idx] += importance - weighted_child_gini
            
            # Continuar recurs√£o
            if node.left:
                traverse(node.left, total_samples)
            if node.right:
                traverse(node.right, total_samples)
        
        if self.root:
            traverse(self.root, len(y))
            
        # Normalizar
        if self.feature_importances_.sum() > 0:
            self.feature_importances_ /= self.feature_importances_.sum()
    
    def predict_sample(self, x):
        node = self.root
        
        while not node.is_leaf():
            if x[node.feature_index] <= node.threshold:
                node = node.left
            else:
                node = node.right
                
        return node.value
    
    def predict(self, X):
        predictions = []
        for x in X:
            predictions.append(self.predict_sample(x))
        return np.array(predictions)

class ManualRandomForest:
    
    def __init__(self, n_estimators=100, max_depth=10, min_samples_split=5, 
                 min_samples_leaf=2, max_features='sqrt', random_state=42):
        self.n_estimators = n_estimators
        self.max_depth = max_depth
        self.min_samples_split = min_samples_split
        self.min_samples_leaf = min_samples_leaf
        self.max_features = max_features
        self.random_state = random_state
        self.trees = []
        self.feature_importances_ = None
        
    def _bootstrap_sample(self, X, y):
        """Cria amostra bootstrap"""
        n_samples = X.shape[0]
        indices = np.random.choice(n_samples, n_samples, replace=True)
        return X[indices], y[indices]
    
    def _get_feature_subset(self, n_features):
        if self.max_features == 'sqrt':
            max_features = int(sqrt(n_features))
        elif self.max_features == 'log2':
            max_features = int(log2(n_features))
        elif isinstance(self.max_features, int):
            max_features = self.max_features
        else:
            max_features = n_features
            
        return np.random.choice(n_features, 
                               min(max_features, n_features), 
                               replace=False)
    
    def fit(self, X, y):
        """Treina o Random Forest"""
        print(f"Treinando Random Forest com {self.n_estimators} √°rvores...")
        
        if self.random_state:
            np.random.seed(self.random_state)
            random.seed(self.random_state)
        
        self.trees = []
        n_features = X.shape[1]
        
        for i in range(self.n_estimators):
            if i % 10 == 0:
                print(f"  Treinando √°rvore {i+1}/{self.n_estimators}")
            
            # Bootstrap sample
            X_bootstrap, y_bootstrap = self._bootstrap_sample(X, y)
            
            # Feature subset
            feature_indices = self._get_feature_subset(n_features)
            X_subset = X_bootstrap[:, feature_indices]
            
            # Treinar √°rvore
            tree = ManualDecisionTree(
                max_depth=self.max_depth,
                min_samples_split=self.min_samples_split,
                min_samples_leaf=self.min_samples_leaf
            )
            
            tree.fit(X_subset, y_bootstrap)
            
            # Armazenar √°rvore com √≠ndices de features
            self.trees.append((tree, feature_indices))
        
        # Calcular import√¢ncias das features
        self._calculate_feature_importances(n_features)
        print("‚úÖ Random Forest treinado com sucesso!")
    
    def _calculate_feature_importances(self, n_features):
        self.feature_importances_ = np.zeros(n_features)
        
        for tree, feature_indices in self.trees:
            for idx, importance in enumerate(tree.feature_importances_):
                original_idx = feature_indices[idx]
                self.feature_importances_[original_idx] += importance
                
        # Normalizar
        self.feature_importances_ /= self.n_estimators
        
        if self.feature_importances_.sum() > 0:
            self.feature_importances_ /= self.feature_importances_.sum()
    
    def predict(self, X):
        predictions = np.zeros((X.shape[0], self.n_estimators))
        
        for i, (tree, feature_indices) in enumerate(self.trees):
            X_subset = X[:, feature_indices]
            predictions[:, i] = tree.predict(X_subset)
        
        # Vota√ß√£o majorit√°ria
        final_predictions = []
        for i in range(X.shape[0]):
            votes = Counter(predictions[i])
            final_predictions.append(votes.most_common(1)[0][0])
            
        return np.array(final_predictions)
    
    def predict_proba(self, X):
        predictions = np.zeros((X.shape[0], self.n_estimators))
        
        for i, (tree, feature_indices) in enumerate(self.trees):
            X_subset = X[:, feature_indices]
            predictions[:, i] = tree.predict(X_subset)
        
        # Calcular probabilidades
        probabilities = np.zeros((X.shape[0], 2))
        
        for i in range(X.shape[0]):
            votes = Counter(predictions[i])
            total_votes = self.n_estimators
            
            probabilities[i, 0] = votes.get(0, 0) / total_votes  # Leg√≠tima
            probabilities[i, 1] = votes.get(1, 0) / total_votes  # Fraude
            
        return probabilities

class FraudDetectionSystem:
    
    def __init__(self):
        self.model = None
        self.feature_names = None
        self.metrics = {}
        self.data_info = {}
        self.X_train = None
        self.X_test = None
        self.y_train = None
        self.y_test = None
        
    def criar_pasta_imagens(self):
        """Cria a pasta imagens se n√£o existir"""
        if not os.path.exists('imagens'):
            os.makedirs('imagens')
            print(" Pasta 'imagens' criada")
    
    def carregar_dados(self, caminho_arquivo):
        """Carrega dados do Credit Card Fraud Dataset"""
        print(f" Carregando Credit Card Fraud Dataset de {caminho_arquivo}...")
        try:
            df = pd.read_csv(caminho_arquivo)
            print(f" Dados carregados: {len(df)} registros, {len(df.columns)} colunas")
            
            # Verificar estrutura esperada do dataset
            expected_columns = ['Time', 'Amount', 'Class'] + [f'V{i}' for i in range(1, 29)]
            
            if 'Class' not in df.columns:
                print(" Coluna 'Class' n√£o encontrada!")
                return None
            
            # Armazenar informa√ß√µes dos dados
            self.data_info = {
                'total_records': len(df),
                'total_features': len(df.columns) - 1,
                'fraud_count': int(df['Class'].sum()),
                'legitimate_count': len(df) - int(df['Class'].sum())
            }
            
            print(f"üìä Transa√ß√µes leg√≠timas: {self.data_info['legitimate_count']:,}")
            print(f"‚ö†Ô∏è Transa√ß√µes fraudulentas: {self.data_info['fraud_count']:,}")
            print(f"üìà Taxa de fraude: {(self.data_info['fraud_count']/self.data_info['total_records'])*100:.3f}%")
            
            return df
            
        except Exception as e:
            print(f"‚ùå Erro ao carregar dados: {str(e)}")
            return None
    
    def analisar_dados(self, df, salvar_arquivo=True, mostrar_plot=False):
        """Analisa e visualiza a distribui√ß√£o das classes"""
        print("üìä Analisando distribui√ß√£o dos dados...")
        self.criar_pasta_imagens()
        
        try:
            # Contar classes
            class_counts = df['Class'].value_counts().sort_index()
            
            # Cria gr√°fico 
            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8), facecolor='#2b2b2b')
            
            # Gr√°fico de barras
            ax1.set_facecolor('#2b2b2b')
            colors = ['#00ff88', '#ff4444']
            labels = ['Leg√≠timas', 'Fraudes']
            
            bars = ax1.bar(labels, class_counts.values, color=colors, alpha=0.8, 
                          edgecolor='white', linewidth=2)
            
            # Adicionar valores
            for bar, count in zip(bars, class_counts.values):
                height = bar.get_height()
                ax1.text(bar.get_x() + bar.get_width()/2, height + max(class_counts.values)*0.02,
                        f'{count:,}', ha='center', va='bottom', 
                        color='white', fontweight='bold', fontsize=14)
            
            ax1.set_title('Distribui√ß√£o de Transa√ß√µes', fontsize=16, fontweight='bold', color='white')
            ax1.set_ylabel('Quantidade', fontsize=12, color='white')
            ax1.tick_params(colors='white')
            
            # Gr√°fico de pizza
            ax2.set_facecolor('#2b2b2b')
            wedges, texts, autotexts = ax2.pie(class_counts.values, labels=labels, colors=colors,
                                             autopct='%1.2f%%', startangle=90)
            
            for text in texts + autotexts:
                text.set_color('white')
                text.set_fontweight('bold')
            
            ax2.set_title('Propor√ß√£o de Classes', fontsize=16, fontweight='bold', color='white')
            
            plt.suptitle('üè¶ AN√ÅLISE DO CREDIT CARD FRAUD DATASET', 
                        fontsize=20, fontweight='bold', color='white', y=0.98)
            
            plt.tight_layout()
            
            if salvar_arquivo:
                save_path = 'imagens/grafico_distribuicao.png'
                plt.savefig(save_path, dpi=300, bbox_inches='tight', 
                           facecolor='#2b2b2b', edgecolor='none')
                print(f"üíæ Gr√°fico salvo em: {save_path}")
            
            if mostrar_plot:
                plt.show()
            else:
                plt.close(fig)
            
            return class_counts
            
        except Exception as e:
            print(f"‚ùå Erro na an√°lise: {str(e)}")
            if 'fig' in locals():
                plt.close(fig)
            return None
    
    def preparar_dados(self, df):
        """Prepara dados para o Random Forest manual"""
        print("‚öôÔ∏è Preparando dados para Random Forest manual...")
        
        try:
            # Separar features e target
            X = df.drop('Class', axis=1).values  # Converter para numpy array
            y = df['Class'].values
            
            # Armazenar nomes das features
            self.feature_names = df.drop('Class', axis=1).columns.tolist()
            print(f"üîß Features: {len(self.feature_names)} ({', '.join(self.feature_names[:5])}...)")
            
            # Normaliza√ß√£o simples (min-max scaling)
            print("üìè Normalizando features...")
            X_min = X.min(axis=0)
            X_max = X.max(axis=0)
            X_range = X_max - X_min
            
            # Evitar divis√£o por zero
            X_range[X_range == 0] = 1
            X_normalized = (X - X_min) / X_range
            
            # Divis√£o treino/teste
            from sklearn.model_selection import train_test_split
            X_train, X_test, y_train, y_test = train_test_split(
                X_normalized, y, test_size=0.2, random_state=42, stratify=y
            )
            
            # Balanceamento para treino (undersampling da classe majorit√°ria)
            print("‚öñÔ∏è Balanceando dados de treino...")
            fraud_indices = np.where(y_train == 1)[0]
            legit_indices = np.where(y_train == 0)[0]
            
            # Manter todas as fraudes e reduzir leg√≠timas
            n_fraud = len(fraud_indices)
            n_legit_sample = min(n_fraud * 3, len(legit_indices))  # Ratio 3:1
            
            legit_sample_indices = np.random.choice(legit_indices, n_legit_sample, replace=False)
            
            # Combinar √≠ndices
            balanced_indices = np.concatenate([fraud_indices, legit_sample_indices])
            np.random.shuffle(balanced_indices)
            
            X_train_balanced = X_train[balanced_indices]
            y_train_balanced = y_train[balanced_indices]
            
            print(f"üìä Dados balanceados - Fraudes: {np.sum(y_train_balanced == 1):,}, "
                  f"Leg√≠timas: {np.sum(y_train_balanced == 0):,}")
            
            # Armazenar para uso posterior
            self.X_train = X_train_balanced
            self.X_test = X_test
            self.y_train = y_train_balanced
            self.y_test = y_test
            
            print(f"‚úÖ Dados preparados - Treino: {len(X_train_balanced):,}, Teste: {len(X_test):,}")
            return X_train_balanced, X_test, y_train_balanced, y_test
            
        except Exception as e:
            print(f"‚ùå Erro na prepara√ß√£o: {str(e)}")
            return None, None, None, None
    
    def treinar_modelo(self, X_train, y_train):
        """Treina o Random Forest manual"""
        print("üå≥ Iniciando treinamento do Random Forest manual...")
        start_time = time.time()
        
        try:
            # Criar e treinar Random Forest manual
            self.model = ManualRandomForest(
                n_estimators=50,  # Reduzido para velocidade
                max_depth=15,
                min_samples_split=10,
                min_samples_leaf=5,
                max_features='sqrt',
                random_state=42
            )
            
            self.model.fit(X_train, y_train)
            
            training_time = time.time() - start_time
            print(f"‚úÖ Random Forest treinado em {training_time:.2f}s!")
            return self.model
            
        except Exception as e:
            print(f"‚ùå Erro no treinamento: {str(e)}")
            return None
    
    def avaliar_modelo(self, X_test, y_test, salvar_arquivo=True, mostrar_plot=False):
        """Avalia o modelo e gera matriz de confus√£o"""
        print("üìè Avaliando Random Forest manual...")
        self.criar_pasta_imagens()
        
        try:
            if self.model is None:
                print("‚ùå Modelo n√£o foi treinado!")
                return None
            
            # Predi√ß√µes
            y_pred = self.model.predict(X_test)
            y_proba = self.model.predict_proba(X_test)
            
            # Calcular m√©tricas manualmente
            # True Positives, False Positives, True Negatives, False Negatives
            tp = np.sum((y_test == 1) & (y_pred == 1))
            fp = np.sum((y_test == 0) & (y_pred == 1))
            tn = np.sum((y_test == 0) & (y_pred == 0))
            fn = np.sum((y_test == 1) & (y_pred == 0))
            
            # M√©tricas
            accuracy = (tp + tn) / (tp + fp + tn + fn) if (tp + fp + tn + fn) > 0 else 0
            precision = tp / (tp + fp) if (tp + fp) > 0 else 0
            recall = tp / (tp + fn) if (tp + fn) > 0 else 0
            f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0
            
            # Armazenar m√©tricas
            self.metrics = {
                'accuracy': float(accuracy),
                'precision': float(precision),
                'recall': float(recall),
                'f1_score': float(f1),
                'tp': int(tp),
                'fp': int(fp),
                'tn': int(tn),
                'fn': int(fn)
            }
            
            # Criar matriz de confus√£o
            cm = np.array([[tn, fp], [fn, tp]])
            
            # Gr√°fico da matriz de confus√£o
            fig, ax = plt.subplots(figsize=(10, 8), facecolor='#2b2b2b')
            ax.set_facecolor('#2b2b2b')
            
            # Heatmap customizado
            im = ax.imshow(cm, interpolation='nearest', cmap='Blues', alpha=0.8)
            
            # Adicionar texto
            for i in range(2):
                for j in range(2):
                    text = ax.text(j, i, f'{cm[i, j]:,}',
                                 ha="center", va="center",
                                 color="white", fontsize=20, fontweight='bold')
            
            # Configura√ß√µes
            ax.set_xticks([0, 1])
            ax.set_yticks([0, 1])
            ax.set_xticklabels(['Leg√≠tima', 'Fraude'], color='white', fontsize=14)
            ax.set_yticklabels(['Leg√≠tima', 'Fraude'], color='white', fontsize=14)
            
            ax.set_xlabel('Predi√ß√£o do Random Forest', fontsize=14, fontweight='bold', color='white')
            ax.set_ylabel('Classe Real', fontsize=14, fontweight='bold', color='white')
            ax.set_title('üéØ MATRIZ DE CONFUS√ÉO - RANDOM FOREST MANUAL', 
                        fontsize=16, fontweight='bold', pad=20, color='white')
            
            # Adicionar m√©tricas
            metrics_text = (f'Acur√°cia: {accuracy:.3f} | Precis√£o: {precision:.3f} | '
                           f'Recall: {recall:.3f} | F1-Score: {f1:.3f}')
            
            plt.figtext(0.5, 0.02, metrics_text, ha='center', fontsize=12, color='white',
                       bbox=dict(boxstyle='round', facecolor='#3498db', alpha=0.8))
            
            plt.tight_layout()
            
            if salvar_arquivo:
                save_path = 'imagens/matriz_confusao.png'
                plt.savefig(save_path, dpi=300, bbox_inches='tight',
                           facecolor='#2b2b2b', edgecolor='none')
                print(f"üíæ Matriz de confus√£o salva em: {save_path}")
            
            if mostrar_plot:
                plt.show()
            else:
                plt.close(fig)
            
            # Imprimir m√©tricas
            print(f"\n M√âTRICAS")
            print(f" Acur√°cia: {accuracy:.4f} ({accuracy*100:.2f}%)")
            print(f" Precis√£o: {precision:.4f} ({precision*100:.2f}%)")
            print(f" Recall: {recall:.4f} ({recall*100:.2f}%)")
            print(f" F1-Score: {f1:.4f}")
            print(f" True Positives: {tp:,}")
            print(f" False Positives: {fp:,}")
            print(f" True Negatives: {tn:,}")
            print(f" False Negatives: {fn:,}")
            print(f"==========================================\n")
            
            return y_pred
            
        except Exception as e:
            print(f" Erro na avalia√ß√£o: {str(e)}")
            if 'fig' in locals():
                plt.close(fig)
            return None
    
    def mostrar_importancia(self, salvar_arquivo=True, mostrar_plot=False, top_n=15):
        """Mostra import√¢ncia das features do Random Forest manual"""
        if self.model is None:
            print(" Modelo n√£o foi treinado.")
            return None
            
        print("Analisando import√¢ncia das features...")
        self.criar_pasta_imagens()
        
        try:
            # Obter import√¢ncias
            importances = self.model.feature_importances_
            feature_importance_pairs = list(zip(self.feature_names, importances))
            feature_importance_pairs.sort(key=lambda x: x[1], reverse=True)
            
            top_features = feature_importance_pairs[:top_n]
            
            # Criar gr√°fico
            fig, ax = plt.subplots(figsize=(14, 10), facecolor='#2b2b2b')
            ax.set_facecolor('#2b2b2b')
            
            features, importance_values = zip(*top_features)
            y_pos = np.arange(len(features))
            
            # Cores gradientes
            colors = plt.cm.viridis(np.linspace(0.2, 0.8, len(features)))
            
            bars = ax.barh(y_pos, importance_values, color=colors, alpha=0.8, 
                          edgecolor='white', linewidth=1)
            
            # Adicionar valores
            for i, (bar, value) in enumerate(zip(bars, importance_values)):
                ax.text(value + max(importance_values)*0.01, bar.get_y() + bar.get_height()/2,
                       f'{value:.4f}', va='center', ha='left', 
                       color='white', fontweight='bold', fontsize=10)
            
            # Configura√ß√µes
            ax.set_yticks(y_pos)
            ax.set_yticklabels(features, color='white', fontsize=11)
            ax.set_xlabel('Import√¢ncia da Feature', fontsize=14, fontweight='bold', color='white')
            ax.set_title(f' TOP {top_n} FEATURES MAIS IMPORTANTES\nRANDOM FOREST MANUAL', 
                        fontsize=16, fontweight='bold', pad=20, color='white')
            
            # Configurar eixos
            ax.tick_params(colors='white')
            ax.spines['bottom'].set_color('white')
            ax.spines['left'].set_color('white')
            ax.spines['top'].set_visible(False)
            ax.spines['right'].set_visible(False)
            
            # Grid sutil
            ax.grid(True, alpha=0.2, color='white', axis='x')
            ax.set_axisbelow(True)
            
            # Inverter ordem para mostrar maior import√¢ncia no topo
            ax.invert_yaxis()
            
            plt.tight_layout()
            
            if salvar_arquivo:
                save_path = 'imagens/importancia.png'
                plt.savefig(save_path, dpi=300, bbox_inches='tight',
                           facecolor='#2b2b2b', edgecolor='none')
                print(f"üíæ Gr√°fico de import√¢ncia salvo em: {save_path}")
            
            if mostrar_plot:
                plt.show()
            else:
                plt.close(fig)
            
            print(" An√°lise de import√¢ncia conclu√≠da.")
            
            # Retornar top 5 para exibi√ß√£o
            return dict(top_features[:5])
            
        except Exception as e:
            print(f" Erro na an√°lise de import√¢ncia: {str(e)}")
            if 'fig' in locals():
                plt.close(fig)
            return None
    
    def gerar_relatorio_completo(self):
        """Gera relat√≥rio completo da an√°lise"""
        if not self.metrics or not self.data_info:
            print(" Execute a an√°lise completa antes de gerar o relat√≥rio!")
            return False
            
        print("üìÑ Gerando relat√≥rio completo...")
        
        try:
            # Obter top 5 features importantes
            top_features = {}
            if self.model and self.feature_names:
                importances = self.model.feature_importances_
                feature_importance_pairs = list(zip(self.feature_names, importances))
                feature_importance_pairs.sort(key=lambda x: x[1], reverse=True)
                top_features = dict(feature_importance_pairs[:5])
            
            total_transacoes = self.data_info['total_records']
            total_fraudes = self.data_info['fraud_count']
            total_legitimas = self.data_info['legitimate_count']
            taxa_fraude = (total_fraudes / total_transacoes) * 100 if total_transacoes > 0 else 0
            
            relatorio = f"""
RELAT√ìRIO COMPLETO - DETEC√á√ÉO DE FRAUDES
 RESUMO DOS DADOS:
‚Ä¢ Dataset: Credit Card Fraud Detection (Kaggle)
‚Ä¢ Total de transa√ß√µes analisadas: {total_transacoes:,}
‚Ä¢ Transa√ß√µes fraudulentas: {total_fraudes:,} ({taxa_fraude:.3f}%)
‚Ä¢ Transa√ß√µes leg√≠timas: {total_legitimas:,} ({100-taxa_fraude:.3f}%)
‚Ä¢ Features analisadas: {self.data_info['total_features']} (V1-V28, Time, Amount)

 MODELO IMPLEMENTADO:
‚Ä¢ Algoritmo: Random Forest Manual (Implementa√ß√£o pr√≥pria)
‚Ä¢ Estrutura: {self.model.n_estimators if self.model else 50} √Årvores de Decis√£o
‚Ä¢ Profundidade m√°xima: {self.model.max_depth if self.model else 15}
‚Ä¢ Crit√©rio de divis√£o: Ganho de Informa√ß√£o (Entropia)
‚Ä¢ Balanceamento: Undersampling (3:1 ratio)
‚Ä¢ Features por √°rvore: ‚àön_features (random subset)

 PERFORMANCE DO MODELO:
‚Ä¢ Acur√°cia: {self.metrics['accuracy']:.4f} ({self.metrics['accuracy']*100:.2f}%)
‚Ä¢ Precis√£o: {self.metrics['precision']:.4f} ({self.metrics['precision']*100:.2f}%)
‚Ä¢ Recall: {self.metrics['recall']:.4f} ({self.metrics['recall']*100:.2f}%)
‚Ä¢ F1-Score: {self.metrics['f1_score']:.4f}

 MATRIZ DE CONFUS√ÉO:
‚Ä¢ True Positives (Fraudes detectadas): {self.metrics.get('tp', 0):,}
‚Ä¢ False Positives (Falsos alarmes): {self.metrics.get('fp', 0):,}
‚Ä¢ True Negatives (Leg√≠timas corretas): {self.metrics.get('tn', 0):,}
‚Ä¢ False Negatives (Fraudes perdidas): {self.metrics.get('fn', 0):,}

TOP 5 FEATURES MAIS IMPORTANTES:
"""
            
            # Adicionar top features ao relat√≥rio
            for i, (feature, importance) in enumerate(top_features.items(), 1):
                relatorio += f"‚Ä¢ {i}¬∫ lugar: {feature} (Import√¢ncia: {importance:.4f})\n"
            
            relatorio += f"""
INTERPRETA√á√ÉO DOS RESULTADOS:
‚Ä¢ Precis√£o: {self.metrics['precision']*100:.1f}% das transa√ß√µes classificadas como fraude s√£o realmente fraudes
‚Ä¢ Recall: {self.metrics['recall']*100:.1f}% das fraudes reais foram detectadas pelo modelo
‚Ä¢ F1-Score: {self.metrics['f1_score']:.3f} indica {'excelente' if self.metrics['f1_score'] > 0.9 else 'bom' if self.metrics['f1_score'] > 0.8 else 'moderado' if self.metrics['f1_score'] > 0.6 else 'baixo'} equil√≠brio entre precis√£o e recall

IMPLEMENTA√á√ÉO T√âCNICA:
‚Ä¢ √Årvores de Decis√£o: Implementa√ß√£o manual com crit√©rio de entropia
‚Ä¢ Divis√£o de n√≥s: Busca exaustiva pelo melhor threshold
‚Ä¢ Parada: Profundidade m√°xima, m√≠nimo de amostras por n√≥
‚Ä¢ Bootstrap: Amostragem com reposi√ß√£o para cada √°rvore
‚Ä¢ Vota√ß√£o: Majorit√°ria para classifica√ß√£o final
‚Ä¢ Probabilidades: Baseadas na propor√ß√£o de votos

ARQUIVOS GERADOS:
‚Ä¢ imagens/grafico_distribuicao.png - Distribui√ß√£o das classes do dataset
‚Ä¢ imagens/matriz_confusao.png - Matriz de confus√£o do Random Forest
‚Ä¢ imagens/importancia.png - Import√¢ncia das features
‚Ä¢ relatorio_fraudes.txt - Este relat√≥rio completo

CARACTER√çSTICAS DO DATASET:
‚Ä¢ V1-V28: Features transformadas por PCA (Principal Component Analysis)
‚Ä¢ Time: Segundos decorridos desde a primeira transa√ß√£o
‚Ä¢ Amount: Valor da transa√ß√£o em euros
‚Ä¢ Class: 0 = Leg√≠tima, 1 = Fraude

RECOMENDA√á√ïES PARA PRODU√á√ÉO:
‚Ä¢ Implementar monitoramento cont√≠nuo de drift nos dados
‚Ä¢ Retreinar modelo periodicamente com novos dados
‚Ä¢ Considerar ensemble com outros algoritmos (SVM, Neural Networks)
‚Ä¢ Implementar pipeline de feature engineering autom√°tico
‚Ä¢ Adicionar explicabilidade local para decis√µes individuais
‚Ä¢ Configurar alertas para transa√ß√µes de alto risco
‚Ä¢ Manter logs detalhados para auditoria e compliance
‚Ä¢ Considerar t√©cnicas de balanceamento mais sofisticadas (SMOTE, ADASYN)

CONSIDERA√á√ïES DE SEGURAN√áA:
‚Ä¢ Validar integridade dos dados de entrada
‚Ä¢ Implementar controles de acesso ao modelo
‚Ä¢ Criptografar dados sens√≠veis em tr√¢nsito e em repouso
‚Ä¢ Manter backup do modelo treinado
‚Ä¢ Implementar rate limiting para preven√ß√£o de ataques

 M√âTRICAS DE NEG√ìCIO:
‚Ä¢ Taxa de Falsos Positivos: {(self.metrics.get('fp', 0) / (self.metrics.get('fp', 0) + self.metrics.get('tn', 1))) * 100:.2f}%
‚Ä¢ Taxa de Falsos Negativos: {(self.metrics.get('fn', 0) / (self.metrics.get('fn', 0) + self.metrics.get('tp', 1))) * 100:.2f}%
‚Ä¢ Especificidade: {(self.metrics.get('tn', 0) / (self.metrics.get('tn', 0) + self.metrics.get('fp', 1))) * 100:.2f}%

 STATUS: {' Modelo apresenta excelente performance para detec√ß√£o de fraudes' if self.metrics['f1_score'] > 0.8 else ' Modelo necessita ajustes nos hiperpar√¢metros'}

=== FIM DO RELAT√ìRIO ===
Gerado automaticamente pelo Sistema de Detec√ß√£o de Fraudes v3.0
Random Forest Manual implementado especificamente para Credit Card Fraud Detection
"""
            
            # Salvar relat√≥rio em arquivo
            with open('relatorio_fraudes.txt', 'w', encoding='utf-8') as f:
                f.write(relatorio)
            print("Relat√≥rio salvo em: relatorio_fraudes.txt")
            
            return True
            
        except Exception as e:
            print(f"‚ùå Erro ao gerar relat√≥rio: {str(e)}")
            return False
    
    def predizer_transacao(self, valores_transacao):
        """Prediz se uma transa√ß√£o espec√≠fica √© fraudulenta"""
        if self.model is None:
            return {"erro": "Modelo n√£o foi treinado ainda!"}
            
        try:
            # Garantir que √© array numpy
            if not isinstance(valores_transacao, np.ndarray):
                valores_transacao = np.array(valores_transacao)
            
            # Reshape se necess√°rio
            if len(valores_transacao.shape) == 1:
                valores_transacao = valores_transacao.reshape(1, -1)
            
            # Normalizar os dados da transa√ß√£o (usar mesmo m√©todo do treino)
            # Nota: Em produ√ß√£o, deveria salvar os par√¢metros de normaliza√ß√£o
            
            # Fazer predi√ß√£o
            predicao = self.model.predict(valores_transacao)[0]
            probabilidades = self.model.predict_proba(valores_transacao)[0]
            
            resultado = {
                'predicao': 'Fraude' if predicao == 1 else 'Leg√≠tima',
                'probabilidade_fraude': float(probabilidades[1]),
                'probabilidade_legitima': float(probabilidades[0]),
                'confianca': float(max(probabilidades)),
                'classe_predita': int(predicao),
                'algoritmo': 'Random Forest Manual'
            }
            
            return resultado
            
        except Exception as e:
            return {"erro": f"Erro na predi√ß√£o: {str(e)}"}
    
    def executar_pipeline_completo(self, caminho_csv, mostrar_plots=False):
        """Executa todo o pipeline de an√°lise"""
        print("Iniciando Sistema de Detec√ß√£o de Fraudes")
        print(Credit Card Fraud Detection com Random Forest Manual")
        print("=" * 60)
        
        try:
            # 1. Carregar dados
            df = self.carregar_dados(caminho_csv)
            if df is None:
                return False
            
            # 2. An√°lise explorat√≥ria
            self.analisar_dados(df, mostrar_plot=mostrar_plots)
            
            # 3. Preparar dados
            X_train, X_test, y_train, y_test = self.preparar_dados(df)
            if X_train is None:
                return False
            
            # 4. Treinar modelo
            self.treinar_modelo(X_train, y_train)
            if self.model is None:
                return False
            
            # 5. Avaliar modelo
            self.avaliar_modelo(X_test, y_test, mostrar_plot=mostrar_plots)
            
            # 6. An√°lise de import√¢ncia
            self.mostrar_importancia(mostrar_plot=mostrar_plots)
            
            # 7. Gerar relat√≥rio completo
            self.gerar_relatorio_completo()
            
            print("\n Pipeline executado com sucesso!")
            print(" Verifique a pasta 'imagens' para os gr√°ficos gerados")
            print(" Verifique o arquivo 'relatorio_fraudes.txt' para o relat√≥rio completo")
            print(" Random Forest Manual treinado e pronto para uso!")
            
            return True
            
        except Exception as e:
            print(f" Erro no pipeline: {str(e)}")
            return False
    
    def demonstracao_dados_sinteticos(self, mostrar_plots=False):
        """Demonstra√ß√£o do sistema com dados sint√©ticos do tipo Credit Card"""
        print(" Executando demonstra√ß√£o com dados sint√©ticos...")
        print(" Simulando Credit Card Fraud Dataset...")
        
        # Criar dados sint√©ticos similares ao dataset real
        np.random.seed(42)
        n_samples = 10000
        n_frauds = 300  # ~3% como no dataset real
        
        print(f Gerando {n_samples} transa√ß√µes sint√©ticas ({n_frauds} fraudes)")
        
        # Gerar features V1-V28 (simulando PCA components)
        legitimate_features = np.random.normal(0, 1, (n_samples - n_frauds, 28))
        fraud_features = np.random.normal(0, 2, (n_frauds, 28))  # Maior vari√¢ncia para fraudes
        
        # Adicionar Time e Amount
        legitimate_time = np.random.uniform(0, 172800, n_samples - n_frauds)  # 48 horas
        fraud_time = np.random.uniform(0, 172800, n_frauds)
        
        legitimate_amount = np.random.lognormal(3, 1.5, n_samples - n_frauds)  # Log-normal para amounts
        fraud_amount = np.random.lognormal(4, 2, n_frauds)  # Amounts maiores para fraudes
        
        # Combinar dados
        X_legit = np.column_stack([legitimate_time, legitimate_features, legitimate_amount])
        X_fraud = np.column_stack([fraud_time, fraud_features, fraud_amount])
        
        X_synthetic = np.vstack([X_legit, X_fraud])
        y_synthetic = np.array([0] * (n_samples - n_frauds) + [1] * n_frauds)
        
        # Criar DataFrame com nomes corretos
        feature_names = ['Time'] + [f'V{i}' for i in range(1, 29)] + ['Amount']
        df_synthetic = pd.DataFrame(X_synthetic, columns=feature_names)
        df_synthetic['Class'] = y_synthetic
        
        # Embaralhar dados
        df_synthetic = df_synthetic.sample(frac=1, random_state=42).reset_index(drop=True)
        
        print(" Executando pipeline com dados sint√©ticos...")
        
        # Armazenar informa√ß√µes dos dados
        self.data_info = {
            'total_records': len(df_synthetic),
            'total_features': len(df_synthetic.columns) - 1,
            'fraud_count': int(df_synthetic['Class'].sum()),
            'legitimate_count': len(df_synthetic) - int(df_synthetic['Class'].sum())
        }
        
        # Executar pipeline
        self.analisar_dados(df_synthetic, mostrar_plot=mostrar_plots)
        X_train, X_test, y_train, y_test = self.preparar_dados(df_synthetic)
        if X_train is not None:
            self.treinar_modelo(X_train, y_train)
            if self.model is not None:
                self.avaliar_modelo(X_test, y_test, mostrar_plot=mostrar_plots)
                self.mostrar_importancia(mostrar_plot=mostrar_plots)
                self.gerar_relatorio_completo()
        
        print(" Demonstra√ß√£o conclu√≠da com sucesso!")
        
        # Exemplo de predi√ß√£o
        print("\n Testando predi√ß√£o com transa√ß√£o sint√©tica...")
        transacao_teste = np.random.normal(0, 1.5, 30)  # Transa√ß√£o suspeita
        resultado = self.predizer_transacao(transacao_teste)
        
        if 'erro' not in resultado:
            print(f" Resultado da predi√ß√£o: {resultado['predicao']}")
            print(f" Probabilidade de fraude: {resultado['probabilidade_fraude']:.3f}")
            print(f" Confian√ßa: {resultado['confianca']:.3f}")
            print(f" Algoritmo: {resultado['algoritmo']}")
        
        return True

def main():
    """Fun√ß√£o principal para teste"""
    # Criar inst√¢ncia do sistema
    fraud_system = FraudDetectionSystem()
    
    print(" Sistema de Detec√ß√£o de Fraudes - Credit Card Dataset")
    print(" Random Forest Manual com √Årvores de Decis√£o Implementadas")
    print(" Especialmente desenvolvido para o dataset do Kaggle")
    print("\n" + "=" * 60)
    
    # CORRIGIDO: Sempre usar dados sint√©ticos realistas se n√£o encontrar arquivo
    print(" Executando com dados sint√©ticos do Credit Card Fraud...")
    fraud_system.demonstracao_dados_sinteticos(mostrar_plots=False)
    
    return fraud_system

if __name__ == "__main__":
    # Executar sistema
    sistema = main()
    
    # Exemplo de uso das funcionalidades
    print("\n" + "="*60)
    print(" EXEMPLO DE USO DO SISTEMA:")
    print("="*60)
    
    print("""
# Para usar o sistema com seu dataset:
sistema = FraudDetectionSystem()

# Carregar e analisar dados reais:
sucesso = sistema.executar_pipeline_completo('creditcard.csv')

# Ou executar demonstra√ß√£o:
sistema.demonstracao_dados_sinteticos(mostrar_plots=True)

# Para fazer predi√ß√µes em novas transa√ß√µes:
# valores = [time, v1, v2, ..., v28, amount]  # 30 features
# resultado = sistema.predizer_transacao(valores)

# Para acessar m√©tricas do modelo:
# print(sistema.metrics)

# Para acessar informa√ß√µes dos dados:
# print(sistema.data_info)

# O modelo √© um Random Forest manual com:
# - √Årvores de Decis√£o implementadas do zero
# - Crit√©rio de divis√£o por ganho de informa√ß√£o
# - Bootstrap sampling para cada √°rvore
# - Vota√ß√£o majorit√°ria para classifica√ß√£o
""")
